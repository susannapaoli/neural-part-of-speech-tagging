{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqxpid8J3_xt"
   },
   "source": [
    "# Programming Assignment 2\n",
    "\n",
    "Welcome to the second programming assignment for CS 4650! In this project, we will train LSTM POS-taggers, which take in a sentence and outputs part-of-speech labels for every word in the sentence.\n",
    "\n",
    "We will use English text from the Wall Street Journal, marked with POS tags such as `NNP` (proper noun) and `DT` (determiner).\n",
    "\n",
    "(Instructor: Wei Xu; TAs: Marcus Ma, Mounica Maddela, Ben Podrazhansky, Rahul Katre)\n",
    "\n",
    "**To begin this project, make a copy of this notebook and save it to your local drive so that you can edit it.**\n",
    "\n",
    "\n",
    "If you want GPU's (which will improve training speed), you can always change your instance type to GPU by going to Runtime -> Change runtime type -> Hardware accelerator.\n",
    "\n",
    "If you're new to PyTorch, or simply want a refresher, we recommend you start by looking through these [Introduction to PyTorch](https://cocoxu.github.io/CS4650_spring2023/slides/PyTorch_tutorial.pdf) slides and this interactive [PyTorch Basics notebook](http://bit.ly/pytorchbasics). Additionally, this [Text Sentiment](http://bit.ly/pytorchexample) notebook will provide some insight into working with PyTorch for NLP specific problems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3X367eCR3_x0"
   },
   "source": [
    "## Part 0 Colab Setup [DO NOT MODIFY]\n",
    "\n",
    "Below, we will import our libraries and check for GPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DtnGNDoA3_x3"
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# this is how we select a GPU if it's avalible on your computer or in the Colab environment.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuEywStkc3M5"
   },
   "source": [
    "You can check to make sure a GPU is available using the following code block.\n",
    "\n",
    "If the below message is shown, it means you are using a CPU.\n",
    "```\n",
    "/bin/bash: nvidia-smi: command not found\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ounnp0ASc58O",
    "outputId": "a11fef5a-80ba-4287-a886-fe2b8741a6f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  8 16:13:07 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   66C    P0    29W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hVu_ia9Ottg",
    "outputId": "22b5f9f5-a919-4196-db08-cd416adf0a6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2775k  100 2775k    0     0  7521k      0 --:--:-- --:--:-- --:--:-- 7521k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/cocoxu/CS4650_projects_spring2023/master/p2_train.txt > train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwA2y6OR3_yE"
   },
   "source": [
    "## Part 1 Data Preparation [10 points]\n",
    "\n",
    "### Part 1.1 Loading Data [DO NOT MODIFY]\n",
    "\n",
    "`train.txt`: The training data is present in this file. This file contains sequences of words and their respective tags. The data is split into 80% training and 20% development to train the model and tune the hyperparameters, respectively. See `load_tag_data` for details on how to read the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kFpH2P1A3_yG",
    "outputId": "da976e7b-449f-46b6-fd93-51badf80f009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Length:  8935\n",
      "Total tags:  44\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY\n",
    "\n",
    "def load_tag_data(tag_file):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    sent = []\n",
    "    tags = []\n",
    "    with open(tag_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                all_sentences.append(sent)\n",
    "                all_tags.append(tags)\n",
    "                sent = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag, _ = line.strip().split()\n",
    "                sent.append(word)\n",
    "                tags.append(tag)\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "train_sentences, train_tags = load_tag_data('train.txt')\n",
    "\n",
    "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
    "\n",
    "# Create train-val split from train data\n",
    "train_val_data = list(zip(train_sentences, train_tags))\n",
    "random.shuffle(train_val_data)\n",
    "\n",
    "print(\"Data Length: \", len(train_val_data))\n",
    "print(\"Total tags: \", len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHTX5AME-Olf"
   },
   "source": [
    "### Part 1.2 Training-Validation Splits\n",
    "\n",
    "We need to split the data into training and validation splits. We will not be using a test split for this project. Implement `train_validation_split` in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NABGfmE0tKNp"
   },
   "outputs": [],
   "source": [
    "# train_validation_split\n",
    "# This method takes in a list of features and labels and splits them into train/val splits.\n",
    "# Note how we are not creating a test set for this project.\n",
    "#\n",
    "# args:\n",
    "# data - list of the tuple (sentence, tags)\n",
    "# labels - list of POS tags for each corresponding sentence\n",
    "# split - split proportion for training and validation\n",
    "#\n",
    "# returns:\n",
    "# train_split, test_split\n",
    "def train_validation_split(data, split=0.8):\n",
    "    train_split, test_split = None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the train-validation split\n",
    "    # Hint: Referencing Project 1 for this function and the subsequent functions\n",
    "    # could prove useful.\n",
    "    #############################################################################\n",
    "    N = len(data)\n",
    "    I = int(N*split)\n",
    "    train_split = data[:I]\n",
    "    test_split = data[I:]\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return train_split, test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Oqod89Q-57B"
   },
   "source": [
    "Testing our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q03jPByI7279",
    "outputId": "9f86de3a-cfa1-465d-9916-ba6d4f0f4e45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data proportion: 0.8\n",
      "Validation data proportion: 0.2\n"
     ]
    }
   ],
   "source": [
    "# testing train_validation_split\n",
    "training_data, val_data = train_validation_split(train_val_data)\n",
    "print(f'Training data proportion: {len(training_data) / len(train_val_data)}')\n",
    "print(f'Validation data proportion: {len(val_data) / len(train_val_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlfliN0J-RzV"
   },
   "source": [
    "### Part 1.3 Word-to-Index and Tag-to-Index mapping\n",
    "In order to work with text in Tensor format, we need to map each word and each tag to a unique index. Implement `create_word_and_tag_dicts` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "44WFle2g9s1J"
   },
   "outputs": [],
   "source": [
    "# create_word_and_tag_dicts\n",
    "# This method takes a collection of sentences and tags and produces three separate\n",
    "# dictionaries that will be used later on.\n",
    "# \n",
    "# args:\n",
    "# data - tuple of (sentences, tags) that we will use to build our dictionary.\n",
    "#\n",
    "# returns:\n",
    "# word_to_idx - dict[str] -> int\n",
    "#        dictionary that maps all of the words in the vocabulary to a unique integer\n",
    "#        representation.\n",
    "#\n",
    "# tag_to_idx - dict[str] -> int\n",
    "#       dictionary that maps each tag to a unique integer representation.\n",
    "#\n",
    "# idx_to_tag - dict[int] -> str\n",
    "#       dictionary that maps each integer from tag_to_idx to its original tag.\n",
    "#       essentially, the inverse of tag_to_idx.\n",
    "def create_word_and_tag_dicts(sentences, unique_tags):\n",
    "    word_to_idx, tag_to_idx, idx_to_tag = {}, {}, {}\n",
    "    #############################################################################\n",
    "    # TODO: Implement create_word_and_tag_dicts\n",
    "    #############################################################################\n",
    "    \n",
    "    # update word_to_idx\n",
    "    w = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "      for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "          word_to_idx[word] = w\n",
    "          w += 1\n",
    "\n",
    "    word_to_idx[' '] = len(word_to_idx)\n",
    "    # update tag_to_idx\n",
    "    t = 0\n",
    "    for tag in unique_tags:\n",
    "      if tag not in tag_to_idx:\n",
    "        tag_to_idx[tag] = t\n",
    "        t += 1\n",
    "\n",
    "    # update idx_to_tag\n",
    "    idx_to_tag = {idx: tag for tag, idx in tag_to_idx.items()}\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return word_to_idx, tag_to_idx, idx_to_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH3bITdO-9RT"
   },
   "source": [
    "Testing our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uojEDun83_yP",
    "outputId": "d36a77f5-1d21-4dab-e00a-9d6209cf19b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags 44\n",
      "Vocab size 19122\n"
     ]
    }
   ],
   "source": [
    "word_to_idx, tag_to_idx, idx_to_tag = create_word_and_tag_dicts(train_sentences, unique_tags)\n",
    "\n",
    "print(\"Total tags\", len(tag_to_idx))\n",
    "print(\"Vocab size\", len(word_to_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl3PapbD_Dbi"
   },
   "source": [
    "### Part 1.4 Prepare Sequence\n",
    "Now we'll put everything together! `prepare_sequence` takes in a sentence and its corresponding tags, and returns the data transformed into index Tensors to be used for training in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "H26dqorp3_yX"
   },
   "outputs": [],
   "source": [
    "# prepare_sequence\n",
    "# This method takes a sentence-tag pair and returns two Long-Tensors of the indices\n",
    "# to be used for the LSTM model.\n",
    "#\n",
    "# returns:\n",
    "# sentence_tensor - torch.LongTensor where each element in the tensor corresponds to\n",
    "# the index of the word in the sentence\n",
    "# tag_tensor - torch.LongTensor where each element in the tensor corresponds to\n",
    "# the index of the tag\n",
    "def prepare_sequence(sentence, tags, word_to_idx, tag_to_idx):\n",
    "    sentence_tensor = torch.empty(len(sentence), dtype=torch.long)\n",
    "    tag_tensor = torch.empty(len(tags), dtype=torch.long)\n",
    "    #############################################################################\n",
    "    # TODO: Implement prepare_sequence\n",
    "    #############################################################################\n",
    "    \n",
    "    # update sentence_tensor\n",
    "    for idx, word in enumerate(sentence):\n",
    "      sentence_tensor[idx] = word_to_idx[word]\n",
    "      \n",
    "    # update tag_tensor\n",
    "    for idx, tag in enumerate(tags):\n",
    "      tag_tensor[idx] = tag_to_idx[tag]\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return sentence_tensor, tag_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRnBTCwD3_yc"
   },
   "source": [
    "## Part 2 Word-Level POS Tagger [20 points]\n",
    "### Part 2.1 Set up model\n",
    "We will build and train a Basic POS Tagger which is an LSTM model to tag the parts of speech in a given sentence using word-level information.\n",
    "\n",
    "\n",
    "First we need to define some default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2P5SHabu3_yf"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 4\n",
    "HIDDEN_DIM = 8\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkkS4oEb3_yk"
   },
   "source": [
    "### Part 2.2 Define Model\n",
    "\n",
    "The model takes as input a sentence as a tensor in the index space. This sentence is then converted to embedding space where each word maps to its word embedding. The word embeddings is learned as part of the model training process. These word embeddings act as input to the LSTM which produces a representation for each word. Then the representations of words are passed to a Linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aCa30HQb3_ym"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "class BasicPOSTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(BasicPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer: that maps words to the embedding space\n",
    "        # an LSTM layer: that takes word embeddings as input and outputs hidden states\n",
    "        # a linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = LSTM_LAYERS, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence as the argument, \n",
    "        # compute the corresponding raw scores for tags (without softmax)\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "        emb = self.embedding(sentence)\n",
    "        self.lstm.flatten_parameters()\n",
    "        lstm_out, (hidden, c) = self.lstm(emb.view(len(sentence), 1, -1))\n",
    "        tag_scores = self.linear(lstm_out.view(len(sentence), -1))\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot9J3MrB3_ys"
   },
   "source": [
    "### Part 2.3 Training\n",
    "\n",
    "We define train and evaluate procedures that allow us to train our model using our created train-val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "BWMGxh4Z3_yv"
   },
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training method\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. Find the gradient with respect to the loss and update the\n",
    "        # model parameters using the optimizer.\n",
    "        #############################################################################\n",
    "        #zero out the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #prepare input data (sentences and gold labels)\n",
    "        prep_sentence, prep_tags = prepare_sequence(sentence, tags, word_to_idx, tag_to_idx)\n",
    "        prep_sentence = prep_sentence.to(device)\n",
    "        prep_tags = prep_tags.to(device)\n",
    "\n",
    "        #do forward pass with current input\n",
    "        output = model(prep_sentence)\n",
    "        \n",
    "        #get loss with model predictions and true labels\n",
    "        loss = loss_function(output, prep_tags)\n",
    "        \n",
    "    \n",
    "        #update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #increase running total loss and the number of past training samples \n",
    "        train_loss += loss.item()\n",
    "        train_examples += len(prep_tags)\n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate(model, loss_function)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "def evaluate(model, loss_function):\n",
    "  # returns:: avg_val_loss (float)\n",
    "  # returns:: val_accuracy (float)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate method\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used.\n",
    "            #############################################################################\n",
    "            \n",
    "            #prepare input data (sentences and gold labels)\n",
    "            prep_sentence, prep_tags = prepare_sequence(sentence, tags, word_to_idx, tag_to_idx)\n",
    "            prep_sentence = prep_sentence.to(device)\n",
    "            prep_tags = prep_tags.to(device)\n",
    "\n",
    "            #do forward pass with current batch of input\n",
    "            output = model(prep_sentence)\n",
    "            \n",
    "            #prep_tags_reshape = prep_tags.view(1, -1)\n",
    "            #output_reshaped = output.view(-1, len(tag_to_idx))\n",
    "            #get loss with model predictions and true labels\n",
    "            loss = loss_function(output, prep_tags)\n",
    "            \n",
    "            #get the predicted labels\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            \n",
    "            #get number of correct prediction\n",
    "            correct += (predictions == prep_tags).sum().item()\n",
    "            #increase running total loss and the number of past valid samples\n",
    "            val_loss += loss.item()\n",
    "            val_examples += len(prep_tags)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lsuHjjH1rQeS",
    "outputId": "193ad95b-7dd5-4dc2-b1e5-bd9cbc5ab9ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\tAvg Train Loss: 1.0072\tAvg Val Loss: 0.8537\t Val Accuracy: 76\n",
      "Epoch: 2/10\tAvg Train Loss: 0.7778\tAvg Val Loss: 0.8329\t Val Accuracy: 78\n",
      "Epoch: 3/10\tAvg Train Loss: 0.6939\tAvg Val Loss: 0.7715\t Val Accuracy: 78\n",
      "Epoch: 4/10\tAvg Train Loss: 0.6938\tAvg Val Loss: 0.7766\t Val Accuracy: 78\n",
      "Epoch: 5/10\tAvg Train Loss: 0.6892\tAvg Val Loss: 0.8474\t Val Accuracy: 78\n",
      "Epoch: 6/10\tAvg Train Loss: 0.6746\tAvg Val Loss: 0.7138\t Val Accuracy: 80\n",
      "Epoch: 7/10\tAvg Train Loss: 0.6674\tAvg Val Loss: 0.7303\t Val Accuracy: 80\n",
      "Epoch: 8/10\tAvg Train Loss: 0.6603\tAvg Val Loss: 0.7634\t Val Accuracy: 78\n",
      "Epoch: 9/10\tAvg Train Loss: 0.6649\tAvg Val Loss: 0.7845\t Val Accuracy: 79\n",
      "Epoch: 10/10\tAvg Train Loss: 0.6444\tAvg Val Loss: 0.7252\t Val Accuracy: 80\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "#############################################################################\n",
    "\n",
    "model = BasicPOSTagger(vocab_size         = len(word_to_idx.keys()),\n",
    "                            embedding_dim = EMBEDDING_DIM,\n",
    "                            hidden_dim    = HIDDEN_DIM,\n",
    "                            tagset_size=len(tag_to_idx.keys())).to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(reduction='sum').to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train(epoch, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK6mT_k8NRvB"
   },
   "source": [
    "**Sanity Check!** Under the default hyperparameter setting, after 5 epochs you should be able to get at least 75% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP64WDReBuDr"
   },
   "source": [
    "### Part 2.4 Error analysis\n",
    "\n",
    "In this step, we will analyze what kind of errors it was making on the validation set.\n",
    "\n",
    "Step 1, write a method to generate predictions from the validation set. For every sentence, get its words, predicted tags (model_tags), and the ground truth tags (gt_tags). To make the next step easier, you may want to concatenate words from all sentences into a very long list, and same for model_tags and gt_tags.\n",
    "\n",
    "\n",
    "Step 2, analyze what kind of errors the model was making. For example, it may frequently label NN as VB. Let's get the top-10 most frequent types of errors, each of their frequency, and some example words. One example is at below. It is interpreted as the model predicts NNP as VBG for 626 times, with five random example words of this error being shown.\n",
    "\n",
    "```\n",
    "['VBG', 'NNP', 626, ['Rowe', 'Livermore', 'Parker', 'F-16', 'HEYNOW']]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QgMHr7HCn1x",
    "outputId": "b385cce9-4df7-4fea-aa87-85bb4217fb98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NNP', 'NN', 623, ['yacht', 'hotdog', 'humor', 'stockpile', 'depressant'])\n",
      "('NNP', 'NNS', 589, ['buddies', 'receipts', 'ounces', 'pleas', 'cities'])\n",
      "('NNP', 'VBN', 580, ['created', 'been', 'priced', 'stopped', 'adjusted'])\n",
      "('NNP', 'VBG', 541, ['declining', 'buying', 'laughing', 'contemplating', 'consuming'])\n",
      "('NNP', 'JJ', 487, ['slick-talking', 'snake-oil', 'gullible', 'Callable', 'ever-narrowing'])\n",
      "('NN', 'JJ', 361, ['greedy', 'uncomfortable', 'own', 'one-year', 'net'])\n",
      "('NNP', 'CD', 308, ['1999', '1994', '101.60', '9.07', '396,000'])\n",
      "('NN', 'VB', 293, ['extract', 'yield', 'yield', 'describe', 'use'])\n",
      "('VBD', 'VBN', 258, ['ended', 'requested', 'revised', 'found', 'Posted'])\n",
      "('NNP', 'IN', 258, ['though', 'that', 'that', 'that', 'that'])\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions for val_data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "# Hint: It should look very similar to the evaluate function.\n",
    "#############################################################################\n",
    "def generate_predictions(model, val_data):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "\n",
    "    word_list = []\n",
    "    gt_tags = []\n",
    "    model_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "\n",
    "            prep_sentence, prep_tags = prepare_sequence(sentence, tags, word_to_idx, tag_to_idx)\n",
    "            prep_sentence = prep_sentence.to(device)\n",
    "            prep_tags = prep_tags.to(device)\n",
    "            output = model(prep_sentence)\n",
    "\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "\n",
    "            for word, tag, pred_tag in zip(sentence, prep_tags, predictions):\n",
    "              word_list.append(word)\n",
    "              gt_tags.append(idx_to_tag[tag.item()])\n",
    "              model_tags.append(idx_to_tag[pred_tag.item()])\n",
    "\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "    frequencies = {}\n",
    "    example_words = {}\n",
    "    for word, model_tag, gt_tag in zip(word_list, model_tags, gt_tags):\n",
    "      if model_tag != gt_tag: \n",
    "        if (model_tag, gt_tag) in frequencies:\n",
    "          frequencies[(model_tag, gt_tag)] += 1\n",
    "          example_words[(model_tag, gt_tag)].append(word)\n",
    "        else:\n",
    "          frequencies[(model_tag, gt_tag)] = 1\n",
    "          example_words[(model_tag, gt_tag)] = [word]\n",
    "    \n",
    "    errors = []\n",
    "    sorted_frequencies = dict(sorted(frequencies.items(), key=lambda x: x[1], reverse=True))\n",
    "    for error in sorted_frequencies:\n",
    "      e = (error[0], error[1], frequencies[error], example_words[error][:5])\n",
    "      errors.append(e)\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "    return errors\n",
    "\n",
    "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
    "errors = error_analysis(word_list, model_tags, gt_tags)\n",
    "\n",
    "for i in errors[:10]:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRNjFRDcD2h7"
   },
   "source": [
    "**Report your findings in the cell below.**  \n",
    "What kinds of errors did the model make and why do you think it made them? Write a short paragraph (4-5 sentences) in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqnTwvdx5BLK"
   },
   "source": [
    "Looking at the printed list, we notice that the most common error is labeling a noun (NN) as a proper noun (NNP). It is interesting noticing that out of the 10 most common errors, 7 involve tagging the word as a plural noun (NNP) instead of something else. The model, thus, learnt in the 'wrong' way how to properly use this tag (maybe due to a particular frequency in the training set).\n",
    "The model also tags some past participle verbs (VBN) as past tense verbs (VBD). This is most likely due to the fact that in English many verbs have the same form in both past participle and past tense cases. I think the character-level POSTagger won't be able to solve this issue. Overall, the model performes a considerable amount of errors, whose quantity might be decreased with the character-level analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLcI9BUZIo04"
   },
   "source": [
    "### Part 2.5 Hyper-parameter Tuning\n",
    "\n",
    "In order to improve your model performance, try making some modifications on `EMBEDDING_DIM`, `HIDDEN_DIM`, and `LEARNING_RATE`. You will receive 50%/75%/100% credit for this section if your model, after being trained for 10 epochs, is able to achieve 80%/85%/90% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RekmpLxzIo04",
    "outputId": "b2ad7c8a-fbcb-42d0-c0c5-12b482f4bed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\tAvg Train Loss: 1.0988\tAvg Val Loss: 0.6504\t Val Accuracy: 83\n",
      "Epoch: 2/10\tAvg Train Loss: 0.4905\tAvg Val Loss: 0.4792\t Val Accuracy: 88\n",
      "Epoch: 3/10\tAvg Train Loss: 0.3493\tAvg Val Loss: 0.4308\t Val Accuracy: 89\n",
      "Epoch: 4/10\tAvg Train Loss: 0.2856\tAvg Val Loss: 0.4034\t Val Accuracy: 90\n",
      "Epoch: 5/10\tAvg Train Loss: 0.2505\tAvg Val Loss: 0.3939\t Val Accuracy: 90\n",
      "Epoch: 6/10\tAvg Train Loss: 0.2280\tAvg Val Loss: 0.3859\t Val Accuracy: 91\n",
      "Epoch: 7/10\tAvg Train Loss: 0.2144\tAvg Val Loss: 0.3836\t Val Accuracy: 91\n",
      "Epoch: 8/10\tAvg Train Loss: 0.2017\tAvg Val Loss: 0.3824\t Val Accuracy: 91\n",
      "Epoch: 9/10\tAvg Train Loss: 0.1941\tAvg Val Loss: 0.3791\t Val Accuracy: 91\n",
      "Epoch: 10/10\tAvg Train Loss: 0.1878\tAvg Val Loss: 0.3816\t Val Accuracy: 91\n"
     ]
    }
   ],
   "source": [
    "YOUR_EMBEDDING_DIM = 2\n",
    "YOUR_HIDDEN_DIM = 6\n",
    "YOUR_LEARNING_RATE = 0.01\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Set three hyper-parameters. Initialize the model, optimizer and the loss function\n",
    "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
    "#############################################################################\n",
    "model = BasicPOSTagger(vocab_size    = len(word_to_idx.keys()),\n",
    "                       embedding_dim = YOUR_EMBEDDING_DIM,\n",
    "                       hidden_dim    = YOUR_HIDDEN_DIM,\n",
    "                       tagset_size   = len(tag_to_idx.keys())).to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(reduction='sum').to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=YOUR_LEARNING_RATE)\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train(epoch, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svXyUssdXZ4r"
   },
   "source": [
    "## Part 3 Character-level POS Tagger  [15 points]\n",
    "\n",
    "Use the character-level information to augment word embeddings. For example, words that end with -ing or -ly give quite a bit of information about their POS tags. To incorporate this information, run a character-level LSTM on every word to create a character-level representation of the word. Take the last hidden state from the character-level LSTM as the representation and concatenate with the word embedding (as in the BasicPOSTagger) to create a new word representation that captures more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nX4-3AoxSJeY"
   },
   "outputs": [],
   "source": [
    "# Create char to index mapping\n",
    "char_to_idx = {}\n",
    "unique_chars = set()\n",
    "MAX_WORD_LEN = 0\n",
    "\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        for c in word:\n",
    "            unique_chars.add(c)\n",
    "        if len(word) > MAX_WORD_LEN:\n",
    "            MAX_WORD_LEN = len(word)\n",
    "\n",
    "for c in unique_chars:\n",
    "    char_to_idx[c] = len(char_to_idx)\n",
    "char_to_idx[' '] = len(char_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xXPsL3nDjAu"
   },
   "source": [
    "## Aside: Padding\n",
    "\n",
    "For this project, we are not coding in batches (as you can see, each training loop runs on a single sentence per iteration). However, padding is a very important aspect of training, so we describe it in  the section below.\n",
    "\n",
    "### How to do padding correctly for the characters?\n",
    "\n",
    "\n",
    "Assume we have got a sentence [\"We\", \"love\", \"NLP\"]. You are supposed to first prepend a certain number of blank characters to each of the words in this sentence.\n",
    "\n",
    "How to determine the number of blank characters we need? The calculation of MAX_WORD_LEN is here for help (which we already provide in the starter code). For the given sentence, MAX_WORD_LEN equals 4. Therefore we prepend two blank characters to \"We\", zero blank character to \"love\", and one blank character to \"NLP\". So the resultant padded sentence we get should be [\"  We\", \"love\", \" NLP\"].\n",
    "\n",
    "Then, we feed all characters in [\"  We\", \"love\", \" NLP\"] into a char-embedding layer, and get a tensor of shape (3, 4, char_embedding_dim). To make this tensor's shape proper for the char-level LSTM (nn.LSTM), we need to transpose this tensor, i.e. swap the first and the second dimension. So we get a tensor of shape (4, 3, char_embedding_dim), where 4 corresponds to seq_len and 3 corresponds to batch_size.\n",
    "\n",
    "The last thing you need to do is to obtain the last hidden state from the char-level LSTM, and concatenate it with the word embedding, so that you can get an augmented representation of that word.\n",
    "\n",
    "[This](https://raw.githubusercontent.com/chaojiang06/chaojiang06.github.io/master/TA/spring2022_CS4650/char_padding.png) is an illustration for left padding characters.\n",
    "\n",
    "### Why doing the padding?\n",
    "Someone may ask why we want to do such a kind of padding, instead of directly passing each of the character sequences of each word one by one through an LSTM, to get the last hidden state. The reason is that if you don't do padding, then that means you can only implement this process using \"for loop\". For CharPOSTagger, if you implement it using \"for loop\", the training time would be approximately 150s (GPU) / 250s (CPU) per epoch, while it would be around 30s (GPU) / 150s (CPU) per epoch if you do the padding and feed your data in batches. Therefore, we strongly recommend you learn how to do the padding and transform your data into batches. In fact, those are quite important concepts which you should get yourself familar with, although it might take you some time.\n",
    "\n",
    "### Why doing left padding?\n",
    "Our hypothesis is that the suffixes of English words (e.g., -ly, -ing, etc) are more indicative than prefixes for the part-of-speech (POS). Though LSTM is supposed to be able to handle long sequences, it still lose information along the way and the information closer to the last state (which you use as char-level representations) will be retained better. \n",
    "\n",
    "### How to understand the dimention change?\n",
    "Assume we have got a sentence with 3 words [\"We\", \"love\", \"NLP\"], and assume the dimension of character embedding is 2, the dimension of word embedding is 4, the dimension of word-level LSTM's hidden layer is 5, the dimension of character-level LSTM's hidden layer is 6.\n",
    "\n",
    "In BasicPOSTagger, the dimension change would be (3x1x4) ----word-level LSTM----> (3x1x5) ----linear layer----> (3x1x44).\n",
    "\n",
    "In CharPOSTagger, after padding, character embedding, and swapping, the dimension change would be (MAX_WORD_LEN, 3, 2) ----character-level LSTM----> (MAX_WORD_LEN, 3, 6) ----Take the last hidden state----> (3, 6) ----concatenate with word embedings----> (3x1x10) ----word-level LSTM----> (3x1x5) ----linear layer----> (3x1x44).\n",
    "\n",
    "### Part 3.1 Define CharPOSTagger Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OMsoXAMDO9-6"
   },
   "outputs": [],
   "source": [
    "# New Hyperparameters\n",
    "EMBEDDING_DIM = 4\n",
    "HIDDEN_DIM = 8\n",
    "LEARNING_RATE = 0.1\n",
    "LSTM_LAYERS = 1\n",
    "DROPOUT = 0\n",
    "EPOCHS = 10\n",
    "CHAR_EMBEDDING_DIM = 4\n",
    "CHAR_HIDDEN_DIM = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7U0wb4OeOsde"
   },
   "outputs": [],
   "source": [
    "class CharPOSTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim, \n",
    "                 char_hidden_dim, char_size, vocab_size, tagset_size):\n",
    "        super(CharPOSTagger, self).__init__()\n",
    "        #############################################################################\n",
    "        # TODO: Define and initialize anything needed for the forward pass.\n",
    "        # You are required to create a model with:\n",
    "        # an embedding layer for word: that maps words to their embedding space\n",
    "        # an embedding layer for character: that maps characters to their embedding space\n",
    "        # a character-level LSTM layer: that finds the character-level embedding for a word\n",
    "        # a word-level LSTM layer: that takes the concatenated representation per word (word embedding + char-lstm) as input and outputs hidden states\n",
    "        # a linear layer: maps from hidden state space to tag space\n",
    "        #############################################################################\n",
    "        self.word_emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.char_emb = nn.Embedding(char_size, char_embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim, num_layers=LSTM_LAYERS)\n",
    "        self.lstm = nn.LSTM(embedding_dim + char_hidden_dim, hidden_dim, num_layers=LSTM_LAYERS, bidirectional = True)\n",
    "        self.linear = nn.Linear(hidden_dim*2, tagset_size)\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "    def forward(self, sentence, chars):\n",
    "        tag_scores = None\n",
    "        #############################################################################\n",
    "        # TODO: Implement the forward pass.\n",
    "        # Given a tokenized index-mapped sentence and a character sequence as the arguments, \n",
    "        # find the corresponding raw scores for tags (without softmax)\n",
    "        # returns:: tag_scores (Tensor)\n",
    "        #############################################################################\n",
    "\n",
    "        word_emb = self.word_emb(sentence)\n",
    "        char_emb = self.char_emb(chars)\n",
    "\n",
    "        char_emb = char_emb.transpose(0,1)\n",
    "\n",
    "        lstm_out_char, (hidden_char, c_char) = self.char_lstm(char_emb)\n",
    "\n",
    "        concatenated = torch.cat([word_emb, hidden_char[-1]], dim=1).unsqueeze(1)\n",
    "        lstm_out_word, (hidden_word, c_word) = self.lstm(concatenated)\n",
    "        tag_scores = self.linear(lstm_out_word)\n",
    "    \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return tag_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PkpZJbiBnh5"
   },
   "source": [
    "### Part 3.2 Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ll3IHzmiSxf6"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_char(epoch, model, loss_function, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_examples = 0\n",
    "    t = 0\n",
    "    for sentence, tags in training_data:\n",
    "        #############################################################################\n",
    "        # TODO: Implement the training method\n",
    "        # Hint: you can use the prepare_sequence method for creating index mappings \n",
    "        # for sentences. For constructing character input, you may want to left pad\n",
    "        # each word to MAX_WORD_LEN first, then use prepare_sequence method to create\n",
    "        # index  mappings. \n",
    "        #############################################################################\n",
    "\n",
    "        #zero out the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #prepare input data (sentences, characters, and gold labels)\n",
    "        prep_sentence, prep_tags = prepare_sequence(sentence, tags, word_to_idx, tag_to_idx)\n",
    "        prep_sentence = prep_sentence.to(device)\n",
    "        prep_tags = prep_tags.to(device)\n",
    "\n",
    "        MAX_WORD_LEN = max(len(word) for word in sentence)\n",
    "        padded_words = [word.rjust(MAX_WORD_LEN) for word in sentence]\n",
    "\n",
    "        chars_seq = []\n",
    "\n",
    "        for idx, word in enumerate(padded_words):\n",
    "          prep_chars, _ = prepare_sequence(word, tags, char_to_idx, tag_to_idx)\n",
    "          prep_chars = prep_chars.to(device)\n",
    "          chars_seq.append(prep_chars)\n",
    "\n",
    "        chars_seq = torch.stack(chars_seq, dim=0)\n",
    "        chars_seq = chars_seq.to(device)\n",
    "        #do forward pass with current batch of input\n",
    "        output = model(prep_sentence, chars_seq)\n",
    "\n",
    "        \n",
    "        output = output.transpose(0,1).squeeze(dim=0)\n",
    "\n",
    "        #get loss with model predictions and true labels\n",
    "        loss = loss_function(output, prep_tags)\n",
    "        #update model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #increase running total loss and the number of past training samples \n",
    "        train_loss += loss.item()\n",
    "        train_examples += len(tags)\n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    avg_train_loss = train_loss / train_examples\n",
    "    avg_val_loss, val_accuracy = evaluate_char(model, loss_function)\n",
    "        \n",
    "    print(\"Epoch: {}/{}\\tAvg Train Loss: {:.4f}\\tAvg Val Loss: {:.4f}\\t Val Accuracy: {:.0f}\".format(epoch, \n",
    "                                                                      EPOCHS, \n",
    "                                                                      avg_train_loss, \n",
    "                                                                      avg_val_loss,\n",
    "                                                                      val_accuracy))\n",
    "\n",
    "\n",
    "def evaluate_char(model, loss_function):\n",
    "    # returns:: avg_val_loss (float)\n",
    "    # returns:: val_accuracy (float)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss = 0\n",
    "    val_examples = 0\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "            #############################################################################\n",
    "            # TODO: Implement the evaluate method\n",
    "            # Find the average validation loss along with the validation accuracy.\n",
    "            # Hint: To find the accuracy, argmax of tag predictions can be used. \n",
    "            #############################################################################\n",
    "\n",
    "            #prepare input data (sentences, characters, and gold labels)\n",
    "            prep_sentence, prep_tags = prepare_sequence(sentence, tags, word_to_idx, tag_to_idx)\n",
    "            prep_sentence = prep_sentence.to(device)\n",
    "            prep_tags = prep_tags.to(device)\n",
    "\n",
    "            MAX_WORD_LEN = max(len(word) for word in sentence)\n",
    "            padded_words = [word.rjust(MAX_WORD_LEN) for word in sentence]\n",
    "\n",
    "            \n",
    "            chars_seq = []\n",
    "            for idx, word in enumerate(padded_words):\n",
    "              prep_chars, _ = prepare_sequence(word, tags, char_to_idx, tag_to_idx)\n",
    "              prep_chars = prep_chars.to(device)\n",
    "              chars_seq.append(prep_chars)\n",
    "        \n",
    "            chars_seq = torch.stack(chars_seq, dim=0)\n",
    "            chars_seq = chars_seq.to(device)\n",
    "            #do forward pass with current batch of input\n",
    "            output = model(prep_sentence, chars_seq)\n",
    "            output = output.transpose(0,1).squeeze(dim=0)\n",
    "            #get loss with model predictions and true labels\n",
    "            loss = loss_function(output, prep_tags)\n",
    "            \n",
    "            #get the predicted labels\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "            \n",
    "            #get number of correct prediction\n",
    "            correct += (predictions == prep_tags).sum().item()\n",
    "\n",
    "            #increase running total loss and the number of past valid samples\n",
    "            val_loss += loss.item()\n",
    "            val_examples += len(prep_tags)\n",
    "            #############################################################################\n",
    "            #                             END OF YOUR CODE                              #\n",
    "            #############################################################################\n",
    "    val_accuracy = 100. * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-QttCw6Otf-",
    "outputId": "8b206c71-ca3e-4a8f-9dee-1d4b91a32b6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\tAvg Train Loss: 0.8123\tAvg Val Loss: 0.5227\t Val Accuracy: 84\n",
      "Epoch: 2/10\tAvg Train Loss: 0.4247\tAvg Val Loss: 0.4590\t Val Accuracy: 87\n",
      "Epoch: 3/10\tAvg Train Loss: 0.3356\tAvg Val Loss: 0.3710\t Val Accuracy: 90\n",
      "Epoch: 4/10\tAvg Train Loss: 0.2977\tAvg Val Loss: 0.3940\t Val Accuracy: 89\n",
      "Epoch: 5/10\tAvg Train Loss: 0.2596\tAvg Val Loss: 0.3599\t Val Accuracy: 90\n",
      "Epoch: 6/10\tAvg Train Loss: 0.2651\tAvg Val Loss: 0.3835\t Val Accuracy: 90\n",
      "Epoch: 7/10\tAvg Train Loss: 0.2666\tAvg Val Loss: 0.5413\t Val Accuracy: 87\n",
      "Epoch: 8/10\tAvg Train Loss: 0.2898\tAvg Val Loss: 0.4527\t Val Accuracy: 89\n",
      "Epoch: 9/10\tAvg Train Loss: 0.2519\tAvg Val Loss: 0.3931\t Val Accuracy: 90\n",
      "Epoch: 10/10\tAvg Train Loss: 0.2304\tAvg Val Loss: 0.3989\t Val Accuracy: 90\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Initialize the model, optimizer and the loss function\n",
    "# Hint, you may want to use reduction='sum' in the CrossEntropyLoss function\n",
    "#############################################################################\n",
    "model = CharPOSTagger(embedding_dim       = EMBEDDING_DIM, \n",
    "                       hidden_dim         = HIDDEN_DIM, \n",
    "                       char_embedding_dim = CHAR_EMBEDDING_DIM, \n",
    "                       char_hidden_dim    = CHAR_HIDDEN_DIM, \n",
    "                       char_size          = len(char_to_idx.keys()), \n",
    "                       vocab_size         = len(word_to_idx.keys()), \n",
    "                       tagset_size        = len(tag_to_idx.keys())).to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss(reduction='sum').to(device) \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "for epoch in range(1, EPOCHS + 1): \n",
    "    train_char(epoch, model, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xslNYW8EBKMQ"
   },
   "source": [
    "**Sanity Check!** Under the default hyperparameter setting, after 5 epochs you should be able to get at least 85% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtsHtaCQIo05"
   },
   "source": [
    "### Part 3.3 Error analysis\n",
    "Write a method to generate predictions for the validation set.\n",
    "Create lists of words, tags predicted by the model and ground truth tags. \n",
    "\n",
    "Then use these lists to carry out error analysis to find the top-10 types of errors made by the model.\n",
    "\n",
    "This part is very similar to part 1.7. You may want to refer to your implementation there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0vUawGsWIo06",
    "outputId": "9d77c45b-ce28-4230-bd03-e8f02aeac98b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NN', 'NNP', 315, ['Bateman', 'Bryan', 'Ivy', 'Wall', 'Wheeling-Pittsburgh'])\n",
      "('NN', 'JJ', 282, ['gullible', 'greedy', 'Callable', 'available', 'capitalist'])\n",
      "('NNP', 'JJ', 226, ['slick-talking', 'snake-oil', 'due', 'due', 'lengthy'])\n",
      "('VBN', 'VBD', 209, ['made', 'permitted', 'conceded', 'continued', 'topped'])\n",
      "('NNP', 'NN', 178, ['hotdog', 'depressant', 'Market', 'tandem', 'duck'])\n",
      "('VBP', 'VB', 119, ['have', 'have', 'have', 'have', 'have'])\n",
      "('NN', 'VB', 108, ['yield', 'describe', 'list', 'EXAMINE', 'boost'])\n",
      "('VBD', 'VBN', 99, ['ended', 'formed', 'held', 'held', 'approved'])\n",
      "('VBZ', 'NNS', 96, ['prices', 'eyebrows', 'switches', 'Thousands', 'prices'])\n",
      "('JJ', 'NN', 93, ['many', 'breakfast', 'wood', 'net', 'defamation'])\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "# TODO: Generate predictions for val_data\n",
    "# Create lists of words, tags predicted by the model and ground truth tags.\n",
    "# Hint: It should look very similar to the evaluate function.\n",
    "#############################################################################\n",
    "def generate_predictions(model, val_data):\n",
    "    # returns:: word_list (str list)\n",
    "    # returns:: model_tags (str list)\n",
    "    # returns:: gt_tags (str list)\n",
    "    # Your code here\n",
    "\n",
    "    word_list = []\n",
    "    gt_tags = []\n",
    "    model_tags = []\n",
    "    with torch.no_grad():\n",
    "        for sentence, tags in val_data:\n",
    "\n",
    "            prep_sentence, prep_tags = prepare_sequence(sentence, tags, word_to_idx, tag_to_idx)\n",
    "            prep_sentence = prep_sentence.to(device)\n",
    "            prep_tags = prep_tags.to(device)\n",
    "\n",
    "            MAX_WORD_LEN = max(len(word) for word in sentence)\n",
    "            padded_words = [word.rjust(MAX_WORD_LEN) for word in sentence]\n",
    "\n",
    "            \n",
    "            chars_seq = []\n",
    "            for idx, word in enumerate(padded_words):\n",
    "              prep_chars, _ = prepare_sequence(word, tags, char_to_idx, tag_to_idx)\n",
    "              prep_chars = prep_chars.to(device)\n",
    "              chars_seq.append(prep_chars)\n",
    "        \n",
    "            chars_seq = torch.stack(chars_seq, dim=0)\n",
    "            chars_seq = chars_seq.to(device)\n",
    "\n",
    "            output = model(prep_sentence, chars_seq)\n",
    "            output = output.transpose(0,1).squeeze(dim=0)\n",
    "\n",
    "            predictions = torch.argmax(output, dim=1)\n",
    "\n",
    "            for word, tag, pred_tag in zip(sentence, prep_tags, predictions):\n",
    "              word_list.append(word)\n",
    "              gt_tags.append(idx_to_tag[tag.item()])\n",
    "              model_tags.append(idx_to_tag[pred_tag.item()])\n",
    "\n",
    "\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "\n",
    "    return word_list, model_tags, gt_tags\n",
    "\n",
    "#############################################################################\n",
    "# TODO: Carry out error analysis\n",
    "# From those lists collected from the above method, find the \n",
    "# top-10 tuples of (model_tag, ground_truth_tag, frequency, example words)\n",
    "# sorted by frequency\n",
    "#############################################################################\n",
    "def error_analysis(word_list, model_tags, gt_tags):\n",
    "    # returns: errors (list of tuples)\n",
    "    # Your code here\n",
    "\n",
    "    frequencies = {}\n",
    "    example_words = {}\n",
    "    for word, model_tag, gt_tag in zip(word_list, model_tags, gt_tags):\n",
    "      if model_tag != gt_tag: \n",
    "        if (model_tag, gt_tag) in frequencies:\n",
    "          frequencies[(model_tag, gt_tag)] += 1\n",
    "          example_words[(model_tag, gt_tag)].append(word)\n",
    "        else:\n",
    "          frequencies[(model_tag, gt_tag)] = 1\n",
    "          example_words[(model_tag, gt_tag)] = [word]\n",
    "    \n",
    "    errors = []\n",
    "    sorted_frequencies = dict(sorted(frequencies.items(), key=lambda x: x[1], reverse=True))\n",
    "    for error in sorted_frequencies:\n",
    "      e = (error[0], error[1], frequencies[error], example_words[error][:5])\n",
    "      errors.append(e)\n",
    "    \n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "\n",
    "    return errors\n",
    "\n",
    "word_list, model_tags, gt_tags = generate_predictions(model, val_data)\n",
    "errors = error_analysis(word_list, model_tags, gt_tags)\n",
    "\n",
    "for i in errors[:10]:\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuLl_BSMeovb"
   },
   "source": [
    "**Report your findings in the cell below.**  \n",
    "What kinds of errors does the character-level model make as compared to the original model, and why do you think it made them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-r8x8hb5lcl"
   },
   "source": [
    "For the character-level POS tagging, the most common error is assigning a noun (NN) tag instead of the proper noun tag (NNP). In this model, thanks to the character-level information, the model might find it easier to match words that the previous model had difficulties with. As predicted, the character-level POSTagger still tags some past tense verbs (VBD) as past participle verbs (VBN) and vice-versa (error 4 and 8). This comes from the fact that due to the same exact forms of these words in English, the model is not able to improve on these mistakes, despite the character-level analysis. Also, we can notice that the model tags some nouns (NN) as proper nouns (NNP), like 'hotdog' or 'Market'. This might be due to either a low frequency of the words or the capitalized first letter, that might refer to the word being at the beginning on the sentence. With the character-level analysis, the amount of error has decreased, with respect to the word-level analysis of the previous tagger. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W09rDJA03pcT"
   },
   "source": [
    "### Part 4: Submit Your Homework\n",
    "This is the end. Congratulations!  \n",
    "\n",
    "Now, follow the steps below to submit your homework in [Gradescope](https://www.gradescope.com/courses/345683):\n",
    "\n",
    "1. Rename this ipynb file to 'CS4650_p2_GTusername.ipynb'. We recommend ensuring you have removed any extraneous cells & print statements, clearing all outputs, and using the Runtime --> Run all tool to make sure all output is update to date. Additionally, leaving comments in your code to help us understand your operations will assist the teaching staff in grading. It is not a requirement, but is recommended. \n",
    "2. Click on the menu 'File' --> 'Download' --> 'Download .py'.\n",
    "3. Click on the menu 'File' --> 'Download' --> 'Download .ipynb'.\n",
    "4. Download the notebook as a .pdf document. Make sure the output from your training loops are captured so we can see how the loss and accuracy changes while training.\n",
    "5. Upload all 3 files to GradeScope.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
